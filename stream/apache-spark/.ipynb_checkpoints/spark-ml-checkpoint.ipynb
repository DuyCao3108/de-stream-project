{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession\\\n",
    "         .builder\\\n",
    "         .master('local')\\\n",
    "         .appName('spark-ml-demo')\\\n",
    "         .getOrCreate()\\\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myclient = pymongo.MongoClient(\"mongodb://172.25.32.171:27017/\",username='duycao',password='123')\n",
    "mydb = myclient['streaming_project']\n",
    "order_col = mydb['orders']\n",
    "item_count_col = mydb['item_counts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate item counts of past 1 hour\n",
    "item_counts = {}\n",
    "\n",
    "for order in order_col.find({\"customer_id\":1}):\n",
    "    for item in order['order_list']:\n",
    "        if item['product_id'] in item_counts:\n",
    "            item_counts[item['product_id']] += 1\n",
    "        else:\n",
    "            item_counts[item['product_id']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing new observation\n",
    "for item in item_counts:\n",
    "    result = item_count_col.find_one({\"product_id\": item})\n",
    "    if result is None:\n",
    "        item_count_col.insert_one({\"product_id\":item, \"historical_counts\": [item_counts[item]]})\n",
    "    else:\n",
    "        result['historical_counts'].append(item_counts[item])\n",
    "        item_count_col.update_one({\"product_id\": item},\n",
    "                           {'$set':result})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate anomaly\n",
    "for item in item_counts:\n",
    "    # get the quantile boundaries\n",
    "    historial_counts = item_count_col.find_one({\"product_id\":item})['historical_counts']\n",
    "    floated_data = [(float(x),) for x in historial_counts]\n",
    "    df = spark.createDataFrame(floated_data, \n",
    "                               schema = StructType([StructField(\"historical_counts\", DoubleType(), True)]))\n",
    "    quan_4th = df.approxQuantile(\"historical_counts\", [0.95], 0)[0]\n",
    "    # comparte quantile bound with actual num\n",
    "    current_count = item_counts[item]\n",
    "    if current_count >= quan_4th:\n",
    "        print(current_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
