{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# you need these two to transform the json strings to dataframes\n",
    "from pyspark.sql.types import MapType,StringType\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "# Spark session & context\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local')\n",
    "         .appName('kafka-new')     \n",
    "         # Add kafka package and mongodb package. Make sure to to this as one string!\n",
    "         # Versions need to match the Spark version (trial & error)\n",
    "         .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.5,org.postgresql:postgresql:42.5.4,org.mongodb.spark:mongo-spark-connector_2.11:2.4.0\")\n",
    "         # Mongo config including the username and password from compose file\n",
    "         .config(\"spark.mongodb.input.uri\",\"mongodb://duycao:123@mongo:27017/streaming_project.orders?authSource=admin\")\n",
    "         .config(\"spark.mongodb.output.uri\",\"mongodb://duycao:123@mongo:27017/streaming_project.orders?authSource=admin\")\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, ArrayType, TimestampType\n",
    "from pyspark.sql.functions import col,from_json, json_tuple, explode, to_json, struct, array, lit, collect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from customer-product from postgres\n",
    "postgres_reader = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://172.25.32.171:5432/postgres\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .option(\"user\", \"postgres\") \\\n",
    "    .option(\"password\", \"example\")\\\n",
    "\n",
    "pg_customer_df = postgres_reader.option(\"dbtable\", \"public.customer\") \\\n",
    "    .load()\n",
    "\n",
    "pg_product_df = postgres_reader.option(\"dbtable\", \"public.product\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the message from the kafka stream\n",
    "kafka_df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "  .option(\"subscribe\", \"api-ingestion\") \\\n",
    "  .load()\n",
    "\n",
    "# convert the binary values to string\n",
    "kafka_df2 = kafka_df.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply schema on kafka df\n",
    "data_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_list\", ArrayType(StructType([\n",
    "        StructField(\"product_id\", IntegerType(), True),\n",
    "        StructField(\"quantity\", IntegerType(), True),\n",
    "        StructField(\"product_category\", StringType(), True),\n",
    "        StructField(\"price\", DoubleType(), True)\n",
    "    ])), True),\n",
    "    StructField(\"order_timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "kafka_df3 = kafka_df2.withColumn(\"json-data\", from_json(\"value\", data_schema)).select(\"json-data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_pg_customer_df = pg_customer_df.withColumnRenamed(\"customer_id\", \"customer_id2\")\n",
    "\n",
    "df_joined_cust = kafka_df3.join(renamed_pg_customer_df, \n",
    "              renamed_pg_customer_df['customer_id2'] == kafka_df3['customer_id'],\n",
    "              how = \"inner\")\n",
    "\n",
    "df_explode = df_joined_cust.select(\"*\", explode(\"order_list\").alias(\"exploded_order_list\"))\n",
    "\n",
    "joined_df = df_explode.join(pg_product_df, df_explode[\"exploded_order_list.product_id\"] == pg_product_df[\"product_id\"], \"left\")\n",
    "\n",
    "# Add the 'name' field to the 'order_list' struct\n",
    "joined_df = joined_df.withColumn(\n",
    "    \"order_list_new\",\n",
    "    array(\n",
    "        struct(\n",
    "            col(\"exploded_order_list.product_id\").alias(\"product_id\"),\n",
    "            col(\"exploded_order_list.quantity\").alias(\"quantity\"),\n",
    "            col(\"price\").alias(\"price\"),\n",
    "            col(\"product_category\").alias(\"product_category\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# # Define the output mode and format\n",
    "# output_query = result_df.writeStream \\\n",
    "#                         .format(\"console\") \\\n",
    "#                         .outputMode(\"complete\") \\\n",
    "#                         .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foreach_batch_function(df, epoch_id):\n",
    "    # Group by customer_id and order_timestamp and collect_list to restore the original order_list format\n",
    "    result_df = df.groupBy(\"customer_id\", \"customer_name\", \"order_timestamp\") \\\n",
    "                     .agg(collect_list(\"order_list_new\").alias(\"order_list\"))\n",
    "    # Write the result to MongoDB using the \"complete\" mode\n",
    "    result_df.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"append\").save()\n",
    "    \n",
    "joined_df.writeStream.foreachBatch(foreach_batch_function).start().awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for query in spark.streams.active:\n",
    "#     query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.streams.active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
